{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6601ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6f97b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit  # pip install numba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dac33f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jit(nopython=True)\n",
    "def run_pde_steps(C, nx, ny, nt, dx, dt, vx, vy, k, source_mask, D):\n",
    "    \"\"\"\n",
    "    Numba-compiled PDE stepper. \n",
    "    Now explicitly accepts D to ensure Soft PINN and Hard PINN match physics.\n",
    "    \"\"\"\n",
    "    for n in range(nt):\n",
    "        C_old = C.copy()\n",
    "        for i in range(1, ny-1):\n",
    "            for j in range(1, nx-1):\n",
    "                # Diffusion (Laplacian)\n",
    "                d2x = (C_old[i, j+1] + C_old[i, j-1] - 2*C_old[i, j]) / dx**2\n",
    "                d2y = (C_old[i+1, j] + C_old[i-1, j] - 2*C_old[i, j]) / dx**2\n",
    "                \n",
    "                # Advection (Upwind Gradient)\n",
    "                # Simple central difference for smoothness, or upwind for stability\n",
    "                grad_x = (C_old[i, j] - C_old[i, j-1]) / dx\n",
    "                grad_y = (C_old[i, j] - C_old[i-1, j]) / dx\n",
    "                \n",
    "                # Discrete Update\n",
    "                C[i, j] = C_old[i, j] + dt * (\n",
    "                    D * (d2x + d2y) -        # Explicit Diffusion\n",
    "                    vx * grad_x -            # Advection X\n",
    "                    vy * grad_y -            # Advection Y\n",
    "                    k * C_old[i, j] +        # Decay\n",
    "                    source_mask[i, j]        # Source Input\n",
    "                )\n",
    "    return C\n",
    "\n",
    "class PDESolver:\n",
    "    def __init__(self, wind_speed, wind_angle, D=0.1, k=0.1):\n",
    "        # NOTE: D=0.1 and v=1.0 gives Pe = vL/D = 10 (Matches Paper's Physics)\n",
    "        self.wind_speed = wind_speed\n",
    "        self.wind_angle = wind_angle\n",
    "        self.D = D \n",
    "        self.k = k\n",
    "        \n",
    "        self.nx = 30\n",
    "        self.ny = 30\n",
    "        self.dx = 1.0 / (self.nx - 1)\n",
    "        \n",
    "        # Velocity Components\n",
    "        self.vx = wind_speed * np.cos(wind_angle * np.pi / 180)\n",
    "        self.vy = wind_speed * np.sin(wind_angle * np.pi / 180)\n",
    "        v_max = np.sqrt(self.vx**2 + self.vy**2)\n",
    "        \n",
    "        # Stability Constraints (CFL)\n",
    "        dt_diff = 0.2 * self.dx**2 / (self.D + 1e-8)\n",
    "        dt_adv = 0.8 * self.dx / (v_max + 1e-8) if v_max > 0 else dt_diff\n",
    "        self.dt = min(dt_diff, dt_adv)\n",
    "        self.nt = int(0.5 / self.dt) # Simulate for 0.5 time units\n",
    "\n",
    "    def solve(self, xs, ys):\n",
    "        C = np.zeros((self.ny, self.nx))\n",
    "        \n",
    "        # Source Term (Gaussian Puff)\n",
    "        x = np.linspace(0, 1, self.nx)\n",
    "        y = np.linspace(0, 1, self.ny)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        sigma = 2 * self.dx\n",
    "        source = np.exp(-((X - xs)**2 + (Y - ys)**2) / (2*sigma**2))\n",
    "        # Normalize source intensity\n",
    "        source = source / (np.sum(source) * self.dx**2 + 1e-10)\n",
    "        \n",
    "        # Run Simulation\n",
    "        C = run_pde_steps(C, self.nx, self.ny, self.nt, self.dx, self.dt, \n",
    "                          self.vx, self.vy, self.k, source, self.D)\n",
    "        \n",
    "        return np.maximum(C, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b793bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# =============================================================================\n",
    "# CORE COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "class PDESolver:\n",
    "\n",
    "    def __init__(self, wind_speed, wind_angle, D=1e-9, k=0.1):\n",
    "        self.wind_speed = wind_speed\n",
    "        self.wind_angle = wind_angle\n",
    "        self.D = D\n",
    "        self.k = k\n",
    "        \n",
    "        self.nx = 30\n",
    "        self.ny = 30\n",
    "        self.dx = 1.0 / (self.nx - 1)\n",
    "        \n",
    "        vx = wind_speed * np.cos(wind_angle * np.pi / 180)\n",
    "        vy = wind_speed * np.sin(wind_angle * np.pi / 180)\n",
    "        v_max = np.sqrt(vx**2 + vy**2)\n",
    "        \n",
    "        dt_diff = 0.2 * self.dx**2\n",
    "        dt_adv = 0.8 * self.dx / (v_max + 1e-10) if v_max > 0 else dt_diff\n",
    "        \n",
    "        self.dt = min(dt_diff, dt_adv)\n",
    "        self.nt = int(0.5 / self.dt)\n",
    "        \n",
    "        self.vx = vx\n",
    "        self.vy = vy\n",
    "        \n",
    "    def solve(self, xs, ys):\n",
    "       \n",
    "        nx, ny, nt = self.nx, self.ny, self.nt\n",
    "        dx, dt = self.dx, self.dt\n",
    "        \n",
    "        C = np.zeros((ny, nx))\n",
    "        \n",
    "        x = np.linspace(0, 1, nx)\n",
    "        y = np.linspace(0, 1, ny)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        \n",
    "        sigma = 3 * dx\n",
    "        source = np.exp(-((X - xs)**2 + (Y - ys)**2) / (2*sigma**2))\n",
    "        source = source / (np.sum(source) * dx**2 + 1e-10)\n",
    "        \n",
    "        for n in range(nt):\n",
    "            C_old = C.copy()\n",
    "            \n",
    "            C[1:-1, 1:-1] = (\n",
    "                C_old[1:-1, 1:-1] +\n",
    "                dt * (\n",
    "                    (C_old[1:-1, 2:] + C_old[1:-1, :-2] - 2*C_old[1:-1, 1:-1]) / dx**2 +\n",
    "                    (C_old[2:, 1:-1] + C_old[:-2, 1:-1] - 2*C_old[1:-1, 1:-1]) / dx**2 -\n",
    "                    self.vx * (C_old[1:-1, 1:-1] - C_old[1:-1, :-2]) / dx -\n",
    "                    self.vy * (C_old[1:-1, 1:-1] - C_old[:-2, 1:-1]) / dx -\n",
    "                    self.k * C_old[1:-1, 1:-1] +\n",
    "                    source[1:-1, 1:-1]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            C[0, :] = C[-1, :] = C[:, 0] = C[:, -1] = 0\n",
    "            C = np.maximum(C, 0)\n",
    "        \n",
    "        return C\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fad91c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorNetwork:\n",
    "    def __init__(self, n_sensors=9):\n",
    "        self.n_sensors = n_sensors\n",
    "        self.locations = []\n",
    "        n_side = int(np.sqrt(n_sensors))\n",
    "        for i in range(n_side):\n",
    "            for j in range(n_side):\n",
    "                self.locations.append([(i+1)/(n_side+1), (j+1)/(n_side+1)])\n",
    "        self.locations = np.array(self.locations)\n",
    "    \n",
    "    def measure(self, C, noise=0.05):\n",
    "        measurements = []\n",
    "        ny, nx = C.shape\n",
    "        for sx, sy in self.locations:\n",
    "            i = int(sx * (nx - 1))\n",
    "            j = int(sy * (ny - 1))\n",
    "            val = C[j, i]\n",
    "            if noise > 0:\n",
    "                val += np.random.normal(0, noise * (np.max(C) + 1e-10))\n",
    "            measurements.append(max(0, val))\n",
    "        return np.array(measurements)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4674294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_dim):\n",
    "        self.W1 = np.random.randn(input_dim, 32) * 0.1\n",
    "        self.b1 = np.zeros(32)\n",
    "        self.W2 = np.random.randn(32, 16) * 0.1\n",
    "        self.b2 = np.zeros(16)\n",
    "        self.W3 = np.random.randn(16, 2) * 0.1\n",
    "        self.b3 = np.zeros(2)\n",
    "        self.trained = False\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def relu(self, x): return np.maximum(0, x)\n",
    "    def sigmoid(self, x): return 1 / (1 + np.exp(-np.clip(x, -10, 10)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(x @ self.W1 + self.b1)\n",
    "        h2 = self.relu(h1 @ self.W2 + self.b2)\n",
    "        return self.sigmoid(h2 @ self.W3 + self.b3)\n",
    "\n",
    "    def train(self, X_train, Y_train, X_val, Y_val, epochs=100, lr=0.01):\n",
    "        self.X_mean = np.mean(X_train, axis=0)\n",
    "        self.X_std = np.std(X_train, axis=0) + 1e-8\n",
    "        self.Y_min = np.min(Y_train, axis=0)\n",
    "        self.Y_max = np.max(Y_train, axis=0)\n",
    "        \n",
    "        X_train_norm = (X_train - self.X_mean) / self.X_std\n",
    "        Y_train_norm = (Y_train - self.Y_min) / (self.Y_max - self.Y_min + 1e-8)\n",
    "        X_val_norm = (X_val - self.X_mean) / self.X_std\n",
    "        Y_val_norm = (Y_val - self.Y_min) / (self.Y_max - self.Y_min + 1e-8)\n",
    "        \n",
    "        batch_size = 32\n",
    "        n_train = len(X_train)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "        # ADD THIS LINE - Learning rate decay\n",
    "            current_lr = lr * (0.95 ** (epoch // 10))\n",
    "            \n",
    "            indices = np.random.permutation(n_train)\n",
    "            X_shuffled = X_train_norm[indices]\n",
    "            Y_shuffled = Y_train_norm[indices]\n",
    "        \n",
    "            epoch_loss = 0\n",
    "            for i in range(0, n_train, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                Y_batch = Y_shuffled[i:i+batch_size]\n",
    "        \n",
    "        \n",
    "                \n",
    "                # Forward\n",
    "                h1 = self.relu(X_batch @ self.W1 + self.b1)\n",
    "                h2 = self.relu(h1 @ self.W2 + self.b2)\n",
    "                out = self.sigmoid(h2 @ self.W3 + self.b3)\n",
    "                \n",
    "                # Backprop\n",
    "                grad = (out - Y_batch) * out * (1 - out)\n",
    "                \n",
    "                dW3 = (h2.T @ grad) / len(X_batch)\n",
    "                db3 = np.sum(grad, axis=0) / len(X_batch)\n",
    "                grad2 = (grad @ self.W3.T) * (h2 > 0)\n",
    "                dW2 = (h1.T @ grad2) / len(X_batch)\n",
    "                db2 = np.sum(grad2, axis=0) / len(X_batch)\n",
    "                grad1 = (grad2 @ self.W2.T) * (h1 > 0)\n",
    "                dW1 = (X_batch.T @ grad1) / len(X_batch)\n",
    "                db1 = np.sum(grad1, axis=0) / len(X_batch)\n",
    "                \n",
    "                #self.W3 -= lr * dW3; self.b3 -= lr * db3\n",
    "                #self.W2 -= lr * dW2; self.b2 -= lr * db2\n",
    "                #self.W1 -= lr * dW1; self.b1 -= lr * db1\n",
    "                self.W3 -= current_lr * dW3; self.b3 -= current_lr * db3\n",
    "                self.W2 -= current_lr * dW2; self.b2 -= current_lr * db2\n",
    "                self.W1 -= current_lr * dW1; self.b1 -= current_lr * db1\n",
    "                epoch_loss += np.mean((out - Y_batch)**2)\n",
    "            \n",
    "            self.train_losses.append(epoch_loss / (n_train/batch_size))\n",
    "            val_out = self.forward(X_val_norm)\n",
    "            self.val_losses.append(np.mean((val_out - Y_val_norm)**2))\n",
    "            \n",
    "            if (epoch+1) % 50 == 0:\n",
    "                print(f\"  Epoch {epoch+1}: Val Loss {self.val_losses[-1]:.5f}\")\n",
    "        self.trained = True\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not self.trained: return np.full((len(X), 2), 0.5)\n",
    "        X_norm = (X - self.X_mean) / self.X_std\n",
    "        Y_norm = self.forward(X_norm)\n",
    "        return Y_norm * (self.Y_max - self.Y_min) + self.Y_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d9255e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_hard:\n",
    "    \"\"\"Direct Inverse Solver (The 'Gold Standard')\"\"\"\n",
    "    def __init__(self, solver, sensors):\n",
    "        self.solver = solver\n",
    "        self.sensors = sensors\n",
    "    \n",
    "    def loss(self, pos, measurements):\n",
    "        xs, ys = pos\n",
    "        if not (0.1 < xs < 0.9 and 0.1 < ys < 0.9): return 1e8\n",
    "        C = self.solver.solve(xs, ys)\n",
    "        pred = self.sensors.measure(C, noise=0)\n",
    "        return np.sum((pred - measurements)**2)\n",
    "    \n",
    "    def predict(self, measurements):\n",
    "        best, best_loss = None, np.inf\n",
    "        # Try multiple starting points to avoid local minima\n",
    "        for x0 in [[0.5, 0.5], [0.2, 0.2], [0.8, 0.8]]:\n",
    "            res = minimize(lambda p: self.loss(p, measurements), x0, \n",
    "                           method='Nelder-Mead', options={'maxiter': 40})\n",
    "            if res.fun < best_loss: best_loss = res.fun; best = res\n",
    "        return best.x if best else np.array([0.5, 0.5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "22eb2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN:\n",
    "    \"\"\"Soft-Constrained PINN (The 'Paper Method')\"\"\"\n",
    "    def __init__(self, solver, sensors, n_collocation=40):\n",
    "        self.solver = solver\n",
    "        self.sensors = sensors\n",
    "        self.n_collocation = n_collocation\n",
    "        #self.lambda_pde = 0.1\n",
    "        self.lambda_pde = 5.0\n",
    "\n",
    "                # Fixed collocation grid (deterministic)\n",
    "        grid_size = int(np.sqrt(n_collocation))\n",
    "        x_coll = np.linspace(0.1, 0.9, grid_size)\n",
    "        y_coll = np.linspace(0.1, 0.9, grid_size)\n",
    "        X_coll, Y_coll = np.meshgrid(x_coll, y_coll)\n",
    "        \n",
    "        # Convert to indices\n",
    "        self.coll_i = (Y_coll * (solver.ny - 1)).astype(int).flatten()\n",
    "        self.coll_j = (X_coll * (solver.nx - 1)).astype(int).flatten()\n",
    "    \n",
    "    def compute_pde_residual(self, C, dx):\n",
    "        \"\"\"Compute PDE residual with error handling\"\"\"\n",
    "        ny, nx = C.shape\n",
    "        \n",
    "        # Ensure we have valid indices\n",
    "        if nx < 3 or ny < 3:\n",
    "            return 0.0  # Can't compute derivatives on tiny grid\n",
    "        \n",
    "        # Random collocation points (avoid boundaries)\n",
    "        try:\n",
    "            cols = np.random.randint(1, nx-1, self.n_collocation)\n",
    "            rows = np.random.randint(1, ny-1, self.n_collocation)\n",
    "        except ValueError:\n",
    "            return 0.0  # Grid too small\n",
    "        \n",
    "        residuals = []\n",
    "        for i, j in zip(rows, cols):\n",
    "            # Check bounds (safety)\n",
    "            if i <= 0 or i >= ny-1 or j <= 0 or j >= nx-1:\n",
    "                continue\n",
    "                \n",
    "            # Compute derivatives\n",
    "            laplacian = (C[i+1,j] + C[i-1,j] + C[i,j+1] + C[i,j-1] - 4*C[i,j]) / (dx**2)\n",
    "            grad_x = (C[i,j+1] - C[i,j-1]) / (2*dx)\n",
    "            grad_y = (C[i+1,j] - C[i-1,j]) / (2*dx)\n",
    "            \n",
    "            # PDE residual: ∂c/∂t = D∇²c - v·∇c - kc\n",
    "            # At steady state, ∂c/∂t ≈ 0, so residual should be ≈ 0\n",
    "            residual = (self.solver.D * laplacian) - \\\n",
    "                    (self.solver.vx * grad_x + self.solver.vy * grad_y) - \\\n",
    "                    (self.solver.k * C[i,j])\n",
    "            \n",
    "            residuals.append(residual**2)\n",
    "        \n",
    "        # Return mean squared residual (or 0 if no valid points)\n",
    "        if len(residuals) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return np.mean(residuals)\n",
    "    def loss(self, pos, measurements):\n",
    "        xs, ys = pos\n",
    "        \n",
    "        # Boundary check\n",
    "        if not (0.1 < xs < 0.9 and 0.1 < ys < 0.9): \n",
    "            return 1e8\n",
    "        \n",
    "        # Solve PDE\n",
    "        try:\n",
    "            C = self.solver.solve(xs, ys)\n",
    "        except Exception:\n",
    "            return 1e8  # If solver fails, return huge loss\n",
    "        \n",
    "        # Data loss\n",
    "        pred = self.sensors.measure(C, noise=0)\n",
    "        L_data = np.sum((pred - measurements)**2)\n",
    "        \n",
    "        # Physics loss\n",
    "        L_pde = self.compute_pde_residual(C, self.solver.dx)\n",
    "        \n",
    "        # Check for None or NaN\n",
    "        if L_pde is None or np.isnan(L_pde):\n",
    "            L_pde = 0.0\n",
    "        \n",
    "        return L_data + self.lambda_pde * L_pde\n",
    "    def predict(self, measurements):\n",
    "        best, best_loss = None, np.inf\n",
    "        for x0 in [[0.5, 0.5], [0.3, 0.7], [0.7, 0.3]]:\n",
    "            res = minimize(lambda p: self.loss(p, measurements), x0, \n",
    "                           method='Nelder-Mead', options={'maxiter': 30})\n",
    "            if res.fun < best_loss: best_loss = res.fun; best = res\n",
    "        return best.x if best else np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b740e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ParticleFilter:\n",
    "    def __init__(self, solver, sensors):\n",
    "        self.solver = solver\n",
    "        self.sensors = sensors\n",
    "        self.n_particles = 50\n",
    "    \n",
    "    def predict(self, measurements):\n",
    "        particles = np.random.uniform(0.2, 0.8, (self.n_particles, 2))\n",
    "        likelihoods = []\n",
    "        for xs, ys in particles:\n",
    "            C = self.solver.solve(xs, ys)\n",
    "            error = np.sum((self.sensors.measure(C, noise=0) - measurements)**2)\n",
    "            likelihoods.append(np.exp(-error / 0.05))\n",
    "        weights = np.array(likelihoods) / (np.sum(likelihoods) + 1e-10)\n",
    "        return np.average(particles, axis=0, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a263499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# LEARNABILITY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_learnability(solver, sensors):\n",
    "    \"\"\"Check if problem is actually learnable\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" LEARNABILITY ANALYSIS \".center(70))\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n1. Sensor Discrimination Test\")\n",
    "    sources = [\n",
    "        (0.3, 0.3), (0.3, 0.7), (0.7, 0.3), (0.7, 0.7),\n",
    "        (0.5, 0.3), (0.5, 0.7), (0.3, 0.5), (0.7, 0.5),\n",
    "        (0.5, 0.5)\n",
    "    ]\n",
    "    \n",
    "    measurements_list = []\n",
    "    for xs, ys in sources:\n",
    "        C = solver.solve(xs, ys)\n",
    "        meas = sensors.measure(C, noise=0)\n",
    "        measurements_list.append(meas)\n",
    "    \n",
    "    n = len(sources)\n",
    "    distances = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dist = np.linalg.norm(measurements_list[i] - measurements_list[j])\n",
    "            distances.append(dist)\n",
    "    \n",
    "    min_dist = np.min(distances)\n",
    "    max_dist = np.max(distances)\n",
    "    mean_dist = np.mean(distances)\n",
    "    \n",
    "    print(f\"  Min distance: {min_dist:.6f}\")\n",
    "    print(f\"  Max distance: {max_dist:.6f}\")\n",
    "    print(f\"  Mean distance: {mean_dist:.6f}\")\n",
    "    \n",
    "    if min_dist / max_dist > 0.1:\n",
    "        print(\"  ✓ Sources well-separated\")\n",
    "    else:\n",
    "        print(\"  ⚠️  Some sources hard to distinguish\")\n",
    "    \n",
    "    print(\"\\n2. Noise Robustness Test\")\n",
    "    test_source = (0.4, 0.6)\n",
    "    C_clean = solver.solve(*test_source)\n",
    "    meas_clean = sensors.measure(C_clean, noise=0)\n",
    "    \n",
    "    for noise_level in [0.01, 0.05, 0.1]:\n",
    "        noisy_samples = []\n",
    "        for _ in range(10):\n",
    "            meas_noisy = sensors.measure(C_clean, noise=noise_level)\n",
    "            noisy_samples.append(meas_noisy)\n",
    "        std_dev = np.std(noisy_samples, axis=0)\n",
    "        snr = np.mean(meas_clean) / (np.mean(std_dev) + 1e-10)\n",
    "        print(f\"  Noise {noise_level*100:.0f}%: SNR = {snr:.2f}\")\n",
    "    \n",
    "    print(\"\\n3. Inverse Problem Uniqueness\")\n",
    "    grid_size = 5\n",
    "    grid_sources = []\n",
    "    grid_measurements = []\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            xs = 0.2 + i * 0.6 / (grid_size - 1)\n",
    "            ys = 0.2 + j * 0.6 / (grid_size - 1)\n",
    "            C = solver.solve(xs, ys)\n",
    "            meas = sensors.measure(C, noise=0)\n",
    "            grid_sources.append([xs, ys])\n",
    "            grid_measurements.append(meas)\n",
    "    \n",
    "    grid_sources = np.array(grid_sources)\n",
    "    grid_measurements = np.array(grid_measurements)\n",
    "    \n",
    "    spatial_dist = np.linalg.norm(\n",
    "        grid_sources[:, None, :] - grid_sources[None, :, :], axis=2\n",
    "    )\n",
    "    meas_dist = np.linalg.norm(\n",
    "        grid_measurements[:, None, :] - grid_measurements[None, :, :], axis=2\n",
    "    )\n",
    "    \n",
    "    mask = spatial_dist > 0\n",
    "    correlation = np.corrcoef(spatial_dist[mask].flatten(), \n",
    "                             meas_dist[mask].flatten())[0, 1]\n",
    "    \n",
    "    print(f\"  Correlation: {correlation:.3f}\")\n",
    "    if correlation > 0.7:\n",
    "        print(\"  ✓ Problem well-posed\")\n",
    "    elif correlation > 0.4:\n",
    "        print(\"  ~ Moderate - learnable but challenging\")\n",
    "    else:\n",
    "        print(\"  ⚠️  Weak - may be ill-posed\")\n",
    "    \n",
    "    return {\n",
    "        'min_distance': min_dist,\n",
    "        'max_distance': max_dist,\n",
    "        'correlation': correlation\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce9d40",
   "metadata": {},
   "source": [
    "### Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f116b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# INDIVIDUAL PLOTTING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_fig1_training_curves(mlp):\n",
    "    \"\"\"Figure 1: Training convergence and overfitting check\"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(mlp.train_losses, label='Training Loss', alpha=0.8, linewidth=2, color='#2E86AB')\n",
    "    plt.plot(mlp.val_losses, label='Validation Loss', alpha=0.8, linewidth=2, color='#A23B72')\n",
    "    plt.xlabel('Epoch', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Loss (MSE)', fontsize=14, fontweight='bold')\n",
    "    plt.title('MLP Training Convergence', fontsize=16, fontweight='bold', pad=15)\n",
    "    plt.legend(fontsize=12, loc='upper right')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig1_Training_Curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig1_Training_Curves.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5af3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UPDATED PLOTTING FUNCTIONS (4-WAY COMPARISON)\n",
    "# =============================================================================\n",
    "\n",
    "def plot_fig2_single_condition_comparison(mlp_errors, pinn_errors, pinn_hard_errors, pf_errors):\n",
    "    \"\"\"Figure 2: Performance comparison including Hard PINN\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    data = [pf_errors, mlp_errors, pinn_errors, pinn_hard_errors]\n",
    "    labels = ['Particle\\nFilter', 'MLP\\n(5k Data)', 'PINN\\n(Soft)', 'PINN\\n(Hard)']\n",
    "    colors = ['#95B8D1', '#FF6B6B', '#4ECDC4', '#45B7D1'] # Hard PINN is darker Cyan/Blue\n",
    "    \n",
    "    # Box plot\n",
    "    bp = ax1.boxplot(data, labels=labels, patch_artist=True, widths=0.6)\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax1.set_ylabel('Localization Error (μm)', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('(a) Error Distribution (Same Condition)', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Bar chart\n",
    "    means = [np.mean(d) for d in data]\n",
    "    stds = [np.std(d) for d in data]\n",
    "    \n",
    "    bars = ax2.bar(labels, means, yerr=stds, capsize=8, width=0.6, \n",
    "                   color=colors, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax2.set_ylabel('Mean Error (μm)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('(b) Mean Performance', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    for bar, mean, std in zip(bars, means, stds):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., mean + std + 0.1,\n",
    "                f'{mean:.1f}±{std:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig2_Single_Condition_Comparison.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig2_Single_Condition_Comparison.png\")\n",
    "\n",
    "def plot_fig5a_generalization_bars(mlp_base, pinn_base, pinn_hard_base, gen_results):\n",
    "    conditions = ['Same (control)'] + list(gen_results.keys())\n",
    "    means = {\n",
    "        'MLP': [np.mean(mlp_base)],\n",
    "        'PINN (Soft)': [np.mean(pinn_base)],\n",
    "        'PINN (Hard)': [np.mean(pinn_hard_base)]\n",
    "    }\n",
    "    stds = {\n",
    "        'MLP': [np.std(mlp_base)],\n",
    "        'PINN (Soft)': [np.std(pinn_base)],\n",
    "        'PINN (Hard)': [np.std(pinn_hard_base)]\n",
    "    }\n",
    "    \n",
    "    for c in gen_results:\n",
    "        means['MLP'].append(gen_results[c]['mlp_mean'])\n",
    "        stds['MLP'].append(gen_results[c]['mlp_std'])\n",
    "        means['PINN (Soft)'].append(gen_results[c]['pinn_mean'])\n",
    "        stds['PINN (Soft)'].append(gen_results[c]['pinn_std'])\n",
    "        means['PINN (Hard)'].append(gen_results[c]['pinn_hard_mean'])\n",
    "        stds['PINN (Hard)'].append(gen_results[c]['pinn_hard_std'])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(conditions))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x - width, means['MLP'], width, yerr=stds['MLP'], label='MLP (Big Data)', \n",
    "            color='#FF6B6B', alpha=0.8, capsize=5, edgecolor='black')\n",
    "    plt.bar(x, means['PINN (Soft)'], width, yerr=stds['PINN (Soft)'], label='PINN (Soft)', \n",
    "            color='#4ECDC4', alpha=0.8, capsize=5, edgecolor='black')\n",
    "    plt.bar(x + width, means['PINN (Hard)'], width, yerr=stds['PINN (Hard)'], label='PINN (Hard)', \n",
    "            color='#45B7D1', alpha=0.8, capsize=5, edgecolor='black')\n",
    "\n",
    "    plt.xlabel('Test Condition', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Error (μm)', fontsize=14, fontweight='bold')\n",
    "    plt.title('Generalization: MLP vs PINN', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(x, conditions, rotation=15)\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig5a_Generalization_Bars.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig5a_Generalization_Bars.png\")\n",
    "\n",
    "def plot_fig5b_generalization_degradation(mlp_base, pinn_base, pinn_hard_base, gen_results):\n",
    "    conditions = ['Same (control)'] + list(gen_results.keys())\n",
    "    m_mlp = [np.mean(mlp_base)] + [gen_results[c]['mlp_mean'] for c in gen_results]\n",
    "    m_hard = [np.mean(pinn_hard_base)] + [gen_results[c]['pinn_hard_mean'] for c in gen_results]\n",
    "    \n",
    "    norm_mlp = np.array(m_mlp) / (m_mlp[0] + 1e-10)\n",
    "    norm_hard = np.array(m_hard) / (m_hard[0] + 1e-10)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(conditions, norm_mlp, 'o-', label='MLP', color='#FF6B6B', lw=3, markersize=10)\n",
    "    plt.plot(conditions, norm_hard, '^-', label='PINN (Hard)', color='#45B7D1', lw=3, markersize=10)\n",
    "    \n",
    "    plt.axhline(1.0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.ylabel('Normalized Error', fontweight='bold')\n",
    "    plt.title('Robustness Analysis', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig5b_Generalization_Degradation.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig5b_Generalization_Degradation.png\")\n",
    "\n",
    "\n",
    "def plot_fig7a_error_vs_distance(true_sources, mlp_e, pinn_e, hard_e, pf_e):\n",
    "    \"\"\"Figure 7a with all 4 methods\"\"\"\n",
    "    center = np.array([0.5, 0.5])\n",
    "    dists = np.linalg.norm(true_sources - center, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(dists, pf_e, alpha=0.6, label='Particle Filter', color='gray')\n",
    "    plt.scatter(dists, pinn_e, alpha=0.6, label='PINN (Soft)', color='#4ECDC4')\n",
    "    plt.scatter(dists, mlp_e, alpha=0.6, label='MLP (5k)', color='#FF6B6B')\n",
    "    plt.scatter(dists, hard_e, alpha=0.8, marker='^', s=80, label='PINN (Hard)', color='#45B7D1', edgecolors='k')\n",
    "    \n",
    "    plt.xlabel('Distance from Center')\n",
    "    plt.ylabel('Error (μm)')\n",
    "    plt.title('Spatial Bias Analysis', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('Fig7a_Error_vs_Distance.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig7a_Error_vs_Distance.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "026c6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_fig3_spatial_error_maps(true_sources, mlp_preds, pinn_preds, \n",
    "                                 mlp_errors, pinn_errors):\n",
    "    \"\"\"Figure 3: Spatial error visualization\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    vmax = max(mlp_errors.max(), pinn_errors.max())\n",
    "    \n",
    "    # MLP\n",
    "    scatter1 = ax1.scatter(true_sources[:, 0], true_sources[:, 1], \n",
    "                          c=mlp_errors, cmap='Reds', s=150, \n",
    "                          edgecolor='black', linewidth=1.5, vmin=0, vmax=vmax)\n",
    "    for i in range(len(true_sources)):\n",
    "        ax1.arrow(true_sources[i, 0], true_sources[i, 1],\n",
    "                 mlp_preds[i, 0] - true_sources[i, 0], \n",
    "                 mlp_preds[i, 1] - true_sources[i, 1],\n",
    "                 color='blue', alpha=0.4, width=0.008, head_width=0.03)\n",
    "    ax1.set_xlabel('X Position', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Y Position', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('(a) Pure ML (MLP) Predictions', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    cbar1 = plt.colorbar(scatter1, ax=ax1, pad=0.02)\n",
    "    cbar1.set_label('Error (μm)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # PINN\n",
    "    scatter2 = ax2.scatter(true_sources[:, 0], true_sources[:, 1], \n",
    "                          c=pinn_errors, cmap='Greens', s=150, \n",
    "                          edgecolor='black', linewidth=1.5, vmin=0, vmax=vmax)\n",
    "    for i in range(len(true_sources)):\n",
    "        ax2.arrow(true_sources[i, 0], true_sources[i, 1],\n",
    "                 pinn_preds[i, 0] - true_sources[i, 0], \n",
    "                 pinn_preds[i, 1] - true_sources[i, 1],\n",
    "                 color='blue', alpha=0.4, width=0.008, head_width=0.03)\n",
    "    ax2.set_xlabel('X Position', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('Y Position', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('(b) Inverse PINN Predictions', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    cbar2 = plt.colorbar(scatter2, ax=ax2, pad=0.02)\n",
    "    cbar2.set_label('Error (μm)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig3_Spatial_Error_Maps.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig3_Spatial_Error_Maps.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ccfa0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_fig4_true_vs_predicted(true_sources, mlp_preds, pinn_preds):\n",
    "    \"\"\"Figure 4: True vs predicted scatter plots\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # MLP\n",
    "    ax1.scatter(true_sources[:, 0], mlp_preds[:, 0], \n",
    "               alpha=0.7, s=80, label='X coordinate', color='#E63946')\n",
    "    ax1.scatter(true_sources[:, 1], mlp_preds[:, 1], \n",
    "               alpha=0.7, s=80, label='Y coordinate', color='#457B9D')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', alpha=0.4, linewidth=2, label='Perfect prediction')\n",
    "    ax1.set_xlabel('True Position', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('MLP Predicted Position', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('(a) Pure ML Performance', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_aspect('equal')\n",
    "    \n",
    "    # PINN\n",
    "    ax2.scatter(true_sources[:, 0], pinn_preds[:, 0], \n",
    "               alpha=0.7, s=80, label='X coordinate', color='#E63946')\n",
    "    ax2.scatter(true_sources[:, 1], pinn_preds[:, 1], \n",
    "               alpha=0.7, s=80, label='Y coordinate', color='#457B9D')\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.4, linewidth=2, label='Perfect prediction')\n",
    "    ax2.set_xlabel('True Position', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('PINN Predicted Position', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title('(b) Inverse PINN Performance', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig4_True_vs_Predicted.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig4_True_vs_Predicted.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b6e05ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_fig6_concentration_field_example(solver, sensors):\n",
    "    \"\"\"Figure 6: Example concentration field with sensor locations\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    examples = [(0.3, 0.3), (0.5, 0.5), (0.7, 0.6)]\n",
    "    titles = ['(a) Source: Lower Left', '(b) Source: Center', '(c) Source: Upper Right']\n",
    "    \n",
    "    for ax, (xs, ys), title in zip(axes, examples, titles):\n",
    "        C = solver.solve(xs, ys)\n",
    "        \n",
    "        im = ax.imshow(C, origin='lower', cmap='hot', extent=[0, 1, 0, 1], \n",
    "                      interpolation='bilinear')\n",
    "        ax.plot(xs, ys, 'g*', markersize=25, markeredgecolor='white', \n",
    "               markeredgewidth=2.5, label='Source', zorder=10)\n",
    "        \n",
    "        for loc in sensors.locations:\n",
    "            ax.plot(loc[0], loc[1], 'wo', markersize=10, markeredgecolor='black', \n",
    "                   markeredgewidth=2, zorder=9)\n",
    "        \n",
    "        ax.set_xlabel('X Position', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('Y Position', fontsize=12, fontweight='bold')\n",
    "        ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "        cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Concentration', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Add legend to first subplot\n",
    "    axes[0].legend(loc='upper left', fontsize=11, framealpha=0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig6_Concentration_Fields.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig6_Concentration_Fields.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c2f54fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig5c_generalization_boxplots(mlp_base, pinn_base, pinn_hard_base, gen_results):\n",
    "    \"\"\"Figure 5c: Error distributions across conditions (4-Way)\"\"\"\n",
    "    # Prepare data structure\n",
    "    data_to_plot = [mlp_base, pinn_base, pinn_hard_base]\n",
    "    labels = ['Same\\n(MLP)', 'Same\\n(Soft)', 'Same\\n(Hard)']\n",
    "    \n",
    "    # Iterate through generalization conditions\n",
    "    for cond in gen_results:\n",
    "        short_name = cond.split('(')[0].strip()\n",
    "        data_to_plot.extend([\n",
    "            gen_results[cond]['mlp_errors'], \n",
    "            gen_results[cond]['pinn_errors'],\n",
    "            gen_results[cond]['pinn_hard_errors']\n",
    "        ])\n",
    "        labels.extend([\n",
    "            f'{short_name}\\n(MLP)', \n",
    "            f'{short_name}\\n(Soft)',\n",
    "            f'{short_name}\\n(Hard)'\n",
    "        ])\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    bp = plt.boxplot(data_to_plot, labels=labels, patch_artist=True, widths=0.7)\n",
    "    \n",
    "    # Color coding: Red (MLP), Teal (Soft), Cyan (Hard)\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1'] * (1 + len(gen_results))\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "        patch.set_edgecolor('black')\n",
    "    \n",
    "    plt.ylabel('Localization Error (μm)', fontsize=14, fontweight='bold')\n",
    "    plt.title('Error Distributions: Big Data MLP vs Physics', fontsize=16, fontweight='bold', pad=15)\n",
    "    plt.grid(True, axis='y', alpha=0.3, linestyle='--')\n",
    "    plt.xticks(fontsize=9, rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig5c_Generalization_Boxplots.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig5c_Generalization_Boxplots.png\")\n",
    "\n",
    "def plot_fig5d_generalization_summary(mlp_base, pinn_base, pinn_hard_base, gen_results):\n",
    "    \"\"\"Figure 5d: Summary table (4-Way)\"\"\"\n",
    "    conditions = ['Same (control)'] + list(gen_results.keys())\n",
    "    \n",
    "    # Collect Means\n",
    "    mlp_means = [np.mean(mlp_base)] + [gen_results[c]['mlp_mean'] for c in gen_results]\n",
    "    hard_means = [np.mean(pinn_hard_base)] + [gen_results[c]['pinn_hard_mean'] for c in gen_results]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Headers\n",
    "    table_data = [['Condition', 'MLP (5k) Error', 'Hard PINN Error', 'PINN Advantage']]\n",
    "    \n",
    "    for i, cond in enumerate(conditions):\n",
    "        mlp_err = f'{mlp_means[i]:.2f} μm'\n",
    "        hard_err = f'{hard_means[i]:.2f} μm'\n",
    "        \n",
    "        # Calculate advantage (avoid divide by zero)\n",
    "        if hard_means[i] > 0.001:\n",
    "            adv_val = mlp_means[i] / hard_means[i]\n",
    "            advantage = f'{adv_val:.1f}×'\n",
    "        else:\n",
    "            advantage = \"∞\"\n",
    "            \n",
    "        table_data.append([cond, mlp_err, hard_err, advantage])\n",
    "    \n",
    "    # Draw Table\n",
    "    table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                     colWidths=[0.3, 0.2, 0.2, 0.15])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    # Styling\n",
    "    for j in range(4):\n",
    "        table[(0, j)].set_facecolor('#2C3E50')\n",
    "        table[(0, j)].set_text_props(color='white', weight='bold')\n",
    "    \n",
    "    for i in range(1, len(table_data)):\n",
    "        # Highlight the \"Advantage\" column\n",
    "        table[(i, 3)].set_facecolor('#FFF9C4')\n",
    "        table[(i, 3)].set_text_props(weight='bold')\n",
    "\n",
    "    plt.title('Performance Summary: MLP vs Hard PINN', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig5d_Generalization_Summary.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig5d_Generalization_Summary.png\")\n",
    "\n",
    "def plot_fig7b_error_cdf(mlp_err, pinn_err, hard_err, pf_err):\n",
    "    \"\"\"Figure 7b: CDF with all 4 methods\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    methods = [\n",
    "        ('Particle Filter', pf_err, 'gray'),\n",
    "        ('MLP (5k Data)', mlp_err, '#FF6B6B'),\n",
    "        ('PINN (Soft)', pinn_err, '#4ECDC4'),\n",
    "        ('PINN (Hard)', hard_err, '#45B7D1')\n",
    "    ]\n",
    "    \n",
    "    for label, data, color in methods:\n",
    "        sorted_data = np.sort(data)\n",
    "        yvals = np.arange(len(sorted_data)) / float(len(sorted_data) - 1)\n",
    "        plt.plot(sorted_data, yvals, label=label, linewidth=3, color=color, alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Error (μm)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Cumulative Probability', fontsize=14, fontweight='bold')\n",
    "    plt.title('Error CDF: Which method is most reliable?', fontsize=16, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('Fig7b_Error_CDF.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig7b_Error_CDF.png\")\n",
    "\n",
    "def plot_fig7c_error_histogram(mlp_err, pinn_err, hard_err, pf_err):\n",
    "    \"\"\"Figure 7c: Histogram with all 4 methods\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Determine bins based on max error\n",
    "    max_val = max(np.max(mlp_err), np.max(hard_err))\n",
    "    bins = np.linspace(0, max_val + 0.5, 15)\n",
    "    \n",
    "    plt.hist(pf_err, bins, alpha=0.3, label='PF', color='gray', edgecolor='black')\n",
    "    plt.hist(mlp_err, bins, alpha=0.5, label='MLP', color='#FF6B6B', edgecolor='black')\n",
    "    # Use 'step' type for PINNs to avoid clutter\n",
    "    plt.hist(pinn_err, bins, histtype='step', linewidth=2, label='Soft PINN', color='#4ECDC4')\n",
    "    plt.hist(hard_err, bins, histtype='step', linewidth=3, label='Hard PINN', color='#45B7D1', linestyle='--')\n",
    "    \n",
    "    plt.xlabel('Error (μm)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Count', fontsize=14, fontweight='bold')\n",
    "    plt.title('Error Histogram', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('Fig7c_Error_Histogram.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig7c_Error_Histogram.png\")\n",
    "\n",
    "def plot_fig7d_statistical_summary(mlp_err, pinn_err, hard_err, pf_err):\n",
    "    \"\"\"Figure 7d: Detailed stats table (4-Way)\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    cols = ['Metric', 'PF', 'MLP', 'Soft PINN', 'Hard PINN']\n",
    "    metrics = ['Mean', 'Median', 'Std Dev', 'Min', 'Max']\n",
    "    \n",
    "    table_data = [cols]\n",
    "    \n",
    "    datasets = [pf_err, mlp_err, pinn_err, hard_err]\n",
    "    \n",
    "    for m in metrics:\n",
    "        row = [m]\n",
    "        for data in datasets:\n",
    "            if m == 'Mean': val = np.mean(data)\n",
    "            elif m == 'Median': val = np.median(data)\n",
    "            elif m == 'Std Dev': val = np.std(data)\n",
    "            elif m == 'Min': val = np.min(data)\n",
    "            elif m == 'Max': val = np.max(data)\n",
    "            row.append(f'{val:.3f}')\n",
    "        table_data.append(row)\n",
    "        \n",
    "    table = ax.table(cellText=table_data, cellLoc='center', loc='center',\n",
    "                     colWidths=[0.2, 0.15, 0.15, 0.2, 0.2])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Headers\n",
    "    for j in range(5):\n",
    "        table[(0, j)].set_facecolor('#2980B9')\n",
    "        table[(0, j)].set_text_props(color='white', weight='bold')\n",
    "        \n",
    "    # Best Performer Highlight (Columns 2,3,4 correspond to MLP, Soft, Hard)\n",
    "    # Row indices 1-5. Let's find best mean (Row 1)\n",
    "    means = [float(table_data[1][i]) for i in range(1,5)]\n",
    "    best_idx = means.index(min(means)) + 1 # +1 offset for label col\n",
    "    table[(1, best_idx)].set_facecolor('#C8E6C9') # Light Green for best mean\n",
    "        \n",
    "    plt.title('Statistical Breakdown', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig7d_Statistical_Summary.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig7d_Statistical_Summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f1bd8fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fig8_uncertainty_analysis(pinn_hard_uncertainties):\n",
    "    \"\"\"Figure 8: Hard PINN uncertainty quantification\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Scatter: X vs Y uncertainty\n",
    "    x_unc = [u[0] for u in pinn_hard_uncertainties]\n",
    "    y_unc = [u[1] for u in pinn_hard_uncertainties]\n",
    "    \n",
    "    ax1.scatter(x_unc, y_unc, alpha=0.6, s=100, c='#45B7D1', edgecolor='black')\n",
    "    ax1.set_xlabel('X Position Uncertainty', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Y Position Uncertainty', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('(a) Uncertainty Distribution', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram of total uncertainty\n",
    "    total_unc = [np.sqrt(u[0]**2 + u[1]**2) for u in pinn_hard_uncertainties]\n",
    "    ax2.hist(total_unc, bins=10, alpha=0.7, color='#45B7D1', edgecolor='black')\n",
    "    ax2.set_xlabel('Total Uncertainty (μm)', fontsize=13, fontweight='bold')\n",
    "    ax2.set_ylabel('Count', fontsize=13, fontweight='bold')\n",
    "    ax2.set_title(f'(b) Mean Uncertainty: {np.mean(total_unc):.3f} μm', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Fig8_Uncertainty_Analysis.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"✓ Saved: Fig8_Uncertainty_Analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb10e6",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05d35063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "              COMPLETE ODOR SOURCE LOCALIZATION ANALYSIS              \n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "                  PHASE 1: SINGLE CONDITION ANALYSIS                  \n",
      "======================================================================\n",
      "\n",
      "Training condition: 1.0 mm/s @ 45°\n",
      "\n",
      "======================================================================\n",
      "                        LEARNABILITY ANALYSIS                         \n",
      "======================================================================\n",
      "\n",
      "1. Sensor Discrimination Test\n",
      "  Min distance: 1.932505\n",
      "  Max distance: 3.198503\n",
      "  Mean distance: 2.648983\n",
      "  ✓ Sources well-separated\n",
      "\n",
      "2. Noise Robustness Test\n",
      "  Noise 1%: SNR = 30.85\n",
      "  Noise 5%: SNR = 6.05\n",
      "  Noise 10%: SNR = 3.03\n",
      "\n",
      "3. Inverse Problem Uniqueness\n",
      "  Correlation: 0.776\n",
      "  ✓ Problem well-posed\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Generating massive dataset (5000 samples)...\n",
      "  500/5000\n",
      "  1000/5000\n",
      "  1500/5000\n",
      "  2000/5000\n",
      "  2500/5000\n",
      "  3000/5000\n",
      "  3500/5000\n",
      "  4000/5000\n",
      "  4500/5000\n",
      "  5000/5000\n",
      "\n",
      "Train: 4000, Val: 1000\n",
      "\n",
      "Training MLP...\n",
      "  Epoch 50: Val Loss 0.00608\n",
      "  Epoch 100: Val Loss 0.00317\n",
      "  Epoch 150: Val Loss 0.00275\n",
      "\n",
      "======================================================================\n",
      "                      VALIDATION LOSS DIAGNOSTIC                      \n",
      "======================================================================\n",
      "\n",
      "1. Last 20 validation loss values:\n",
      "[np.float64(0.0028662400277472866), np.float64(0.002859306996876881), np.float64(0.0028524477588650807), np.float64(0.002845692648606532), np.float64(0.00283900436056402), np.float64(0.0028323584419858924), np.float64(0.002825776847983259), np.float64(0.0028193716569988865), np.float64(0.002812983797030943), np.float64(0.002806676532530055), np.float64(0.002800758290813699), np.float64(0.0027949250732040793), np.float64(0.002789082344234882), np.float64(0.002783351551469196), np.float64(0.0027776682408631724), np.float64(0.0027720160685186157), np.float64(0.002766377202773578), np.float64(0.002760863487334979), np.float64(0.002755404020918304), np.float64(0.0027499876120721167)]\n",
      "   Min: 0.00274999\n",
      "   Max: 0.08120079\n",
      "   Range: 0.07845080\n",
      "\n",
      "2. Validation predictions analysis:\n",
      "   Prediction means: [0.44139546 0.44337326]\n",
      "   Prediction stds: [0.18453046 0.1118947 ]\n",
      "   First 5 predictions:\n",
      "     Sample 0: [0.70153409 0.49236172]\n",
      "     Sample 1: [0.4631975  0.43771233]\n",
      "     Sample 2: [0.25534163 0.69262256]\n",
      "     Sample 3: [0.72571803 0.49995608]\n",
      "     Sample 4: [0.3620503  0.29199158]\n",
      "\n",
      "3. Prediction diversity (rounded to 4 decimals):\n",
      "   Unique X predictions: 10/10\n",
      "   Unique Y predictions: 10/10\n",
      "\n",
      "4. Output layer analysis:\n",
      "   W3 mean: 0.029363, std: 0.312869\n",
      "   b3 values: [-0.07357237 -0.08771717]\n",
      "   Output logits for center input would be: [-0.07357237 -0.08771717]\n",
      "   Output sigmoid of logits: [0.4816152  0.47808476]\n",
      "\n",
      "5. Generating high-resolution validation loss plot...\n",
      "   Saved: Diagnostic_Validation_Loss.png\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Testing on 20 held-out samples...\n",
      "\n",
      "DEBUG Sample 0:\n",
      "  Measurements: [0.         0.0283753  0.17942096 0.37215322 0.90048134 0.59021223\n",
      " 0.666613   2.30894359 0.85157033]\n",
      "  True source: (0.682, 0.517)\n",
      "  Test residual: 3.7178604692330737\n",
      "\n",
      "DEBUG Sample 1:\n",
      "  Measurements: [0.33200568 1.56029363 1.1555675  0.23597864 0.89731291 0.82228416\n",
      " 0.22028616 0.13937105 0.219353  ]\n",
      "  True source: (0.271, 0.584)\n",
      "  Test residual: 4.997334232013644\n",
      "\n",
      "DEBUG Sample 2:\n",
      "  Measurements: [1.39510462 1.77284026 0.52978053 0.56615357 0.84593864 0.33773885\n",
      " 0.23308681 0.19319361 0.11761546]\n",
      "  True source: (0.255, 0.399)\n",
      "  Test residual: 11.156401008564908\n",
      "\n",
      "DEBUG Sample 3:\n",
      "  Measurements: [0.50916474 0.69972433 0.70301927 0.57747723 2.03273231 1.03737228\n",
      " 0.39654463 0.77144824 0.35300776]\n",
      "  True source: (0.456, 0.533)\n",
      "  Test residual: 54.94627411728644\n",
      "\n",
      "DEBUG Sample 4:\n",
      "  Measurements: [0.         0.18419906 0.40151848 0.14671165 1.13964772 1.45289699\n",
      " 0.18929232 0.96324619 1.00671911]\n",
      "  True source: (0.577, 0.618)\n",
      "  Test residual: 29.755691438641684\n",
      "  5/20\n",
      "\n",
      "DEBUG Sample 5:\n",
      "  Measurements: [0.0112736  0.31413452 0.03782998 0.94427799 0.27264949 0.1402456\n",
      " 1.8198445  0.78330758 0.2058737 ]\n",
      "  True source: (0.674, 0.279)\n",
      "  Test residual: 11.597374837509832\n",
      "\n",
      "DEBUG Sample 6:\n",
      "  Measurements: [0.87507088 0.86983227 0.29504642 1.85593446 1.05534963 0.08371201\n",
      " 0.26394605 0.2592448  0.00302746]\n",
      "  True source: (0.406, 0.321)\n",
      "  Test residual: 39.00936255272863\n",
      "\n",
      "DEBUG Sample 7:\n",
      "  Measurements: [0.19025165 0.         0.         1.18460105 0.366797   0.24877744\n",
      " 1.68673692 0.54563427 0.04807237]\n",
      "  True source: (0.624, 0.220)\n",
      "  Test residual: 20.878414120399402\n",
      "\n",
      "DEBUG Sample 8:\n",
      "  Measurements: [0.         0.         0.         0.22362066 0.60694671 0.29730516\n",
      " 0.80643982 2.20173016 0.30138943]\n",
      "  True source: (0.746, 0.443)\n",
      "  Test residual: 15.440863838226392\n",
      "\n",
      "DEBUG Sample 9:\n",
      "  Measurements: [0.07537033 0.31919083 0.06214224 0.49843825 1.19792042 0.35474422\n",
      " 0.62523882 1.84229133 0.6966089 ]\n",
      "  True source: (0.656, 0.484)\n",
      "  Test residual: 25.94184052155735\n",
      "  10/20\n",
      "\n",
      "DEBUG Sample 10:\n",
      "  Measurements: [0.36049711 0.75807534 1.51571193 0.20458499 1.12830278 1.34949756\n",
      " 0.15726064 0.14965438 0.3835255 ]\n",
      "  True source: (0.372, 0.651)\n",
      "  Test residual: 16.10212031436854\n",
      "\n",
      "DEBUG Sample 11:\n",
      "  Measurements: [1.03743627 2.11490474 0.64484332 0.58839387 0.77461305 0.43172136\n",
      " 0.04802791 0.26495621 0.26406893]\n",
      "  True source: (0.258, 0.447)\n",
      "  Test residual: 41.93742596213947\n",
      "\n",
      "DEBUG Sample 12:\n",
      "  Measurements: [0.7446655  1.4498354  0.49574396 0.95828236 1.802826   0.67090312\n",
      " 0.2310977  0.46083644 0.32236591]\n",
      "  True source: (0.369, 0.434)\n",
      "  Test residual: 28.261439738433914\n",
      "\n",
      "DEBUG Sample 13:\n",
      "  Measurements: [0.0522848  0.20455687 0.21902631 0.69884613 0.54893494 0.\n",
      " 2.18505676 0.72784523 0.        ]\n",
      "  True source: (0.723, 0.249)\n",
      "  Test residual: 2.3452958017231786\n",
      "\n",
      "DEBUG Sample 14:\n",
      "  Measurements: [0.19458456 0.64192828 0.43080168 0.43818022 2.15473915 1.08411672\n",
      " 0.36324579 0.91291163 0.60583184]\n",
      "  True source: (0.535, 0.529)\n",
      "  Test residual: 14.943720860763316\n",
      "  15/20\n",
      "\n",
      "DEBUG Sample 15:\n",
      "  Measurements: [0.         0.16050927 1.14685498 0.24223379 0.49072643 1.6236109\n",
      " 0.         0.24297565 0.31905855]\n",
      "  True source: (0.399, 0.784)\n",
      "  Test residual: 3.800558972460462\n",
      "\n",
      "DEBUG Sample 16:\n",
      "  Measurements: [0.62910592 1.50882127 0.55446792 0.66571232 1.5937302  0.5710686\n",
      " 0.20138623 0.46430235 0.35856233]\n",
      "  True source: (0.372, 0.505)\n",
      "  Test residual: 13.67569356138082\n",
      "\n",
      "DEBUG Sample 17:\n",
      "  Measurements: [1.90309828 1.18706867 0.2791302  0.88758951 0.63760984 0.17110292\n",
      " 0.11950798 0.19372774 0.10662837]\n",
      "  True source: (0.289, 0.318)\n",
      "  Test residual: 13.578010268964528\n",
      "\n",
      "DEBUG Sample 18:\n",
      "  Measurements: [0.16853455 0.08790441 0.0307784  0.81090563 0.27955662 0.24010974\n",
      " 1.97953351 0.77238276 0.25768451]\n",
      "  True source: (0.704, 0.202)\n",
      "  Test residual: 26.97548374931218\n",
      "\n",
      "DEBUG Sample 19:\n",
      "  Measurements: [0.10744572 0.25366025 0.39921627 0.17782186 0.44860315 0.93372197\n",
      " 0.05109064 0.85461555 2.0496227 ]\n",
      "  True source: (0.670, 0.700)\n",
      "  Test residual: 36.06284197846169\n",
      "  20/20\n",
      "\n",
      "Hard PINN Uncertainty Analysis:\n",
      "  Mean X uncertainty: 0.0001\n",
      "  Mean Y uncertainty: 0.0001\n",
      "\n",
      "Results on SAME condition:\n",
      "  PF:   0.4 ± 0.3 μm\n",
      "  MLP:  0.4 ± 0.2 μm\n",
      "  PINN: 1.6 ± 0.9 μm\n",
      "  PINN: 0.2 ± 0.1 μm\n",
      "\n",
      "======================================================================\n",
      "                     PHASE 2: GENERALIZATION TEST                     \n",
      "======================================================================\n",
      "\n",
      "Higher wind (50%): 1.5 mm/s @ 45°\n",
      "  5/20\n",
      "  5/20 | Hard PINN unc: ±0.000 μm\n",
      "  10/20\n",
      "  10/20 | Hard PINN unc: ±0.000 μm\n",
      "  15/20\n",
      "  15/20 | Hard PINN unc: ±0.000 μm\n",
      "  20/20\n",
      "  20/20 | Hard PINN unc: ±0.000 μm\n",
      "Results:\n",
      "  MLP:  0.4 ± 0.2 μm\n",
      "  PINN: 1.6 ± 0.9 μm\n",
      "  PINN is 0.2× better!\n",
      "Results:\n",
      "  MLP:  0.4 ± 0.2 μm\n",
      "  PINN Hard: 0.2 ± 0.1 μm\n",
      "  PINN Hard is 2.2× better!\n",
      "\n",
      "Different angle: 1.0 mm/s @ 30°\n",
      "  5/20\n",
      "  5/20 | Hard PINN unc: ±0.000 μm\n",
      "  10/20\n",
      "  10/20 | Hard PINN unc: ±0.000 μm\n",
      "  15/20\n",
      "  15/20 | Hard PINN unc: ±0.000 μm\n",
      "  20/20\n",
      "  20/20 | Hard PINN unc: ±0.000 μm\n",
      "Results:\n",
      "  MLP:  0.4 ± 0.2 μm\n",
      "  PINN: 1.6 ± 1.0 μm\n",
      "  PINN is 0.3× better!\n",
      "Results:\n",
      "  MLP:  0.4 ± 0.2 μm\n",
      "  PINN Hard: 0.2 ± 0.1 μm\n",
      "  PINN Hard is 2.2× better!\n",
      "\n",
      "Both changed: 1.2 mm/s @ 60°\n",
      "  5/20\n",
      "  5/20 | Hard PINN unc: ±0.000 μm\n",
      "  10/20\n",
      "  10/20 | Hard PINN unc: ±0.000 μm\n",
      "  15/20\n",
      "  15/20 | Hard PINN unc: ±0.000 μm\n",
      "  20/20\n",
      "  20/20 | Hard PINN unc: ±0.000 μm\n",
      "Results:\n",
      "  MLP:  0.3 ± 0.2 μm\n",
      "  PINN: 1.6 ± 0.9 μm\n",
      "  PINN is 0.2× better!\n",
      "Results:\n",
      "  MLP:  0.3 ± 0.2 μm\n",
      "  PINN Hard: 0.2 ± 0.1 μm\n",
      "  PINN Hard is 2.1× better!\n",
      "\n",
      "======================================================================\n",
      "                    GENERATING PUBLICATION FIGURES                    \n",
      "======================================================================\n",
      "\n",
      "✓ Saved: Fig1_Training_Curves.png\n",
      "✓ Saved: Fig2_Single_Condition_Comparison.png\n",
      "✓ Saved: Fig3_Spatial_Error_Maps.png\n",
      "✓ Saved: Fig4_True_vs_Predicted.png\n",
      "✓ Saved: Fig5a_Generalization_Bars.png\n",
      "✓ Saved: Fig5b_Generalization_Degradation.png\n",
      "✓ Saved: Fig5c_Generalization_Boxplots.png\n",
      "✓ Saved: Fig5d_Generalization_Summary.png\n",
      "✓ Saved: Fig6_Concentration_Fields.png\n",
      "✓ Saved: Fig7a_Error_vs_Distance.png\n",
      "✓ Saved: Fig7b_Error_CDF.png\n",
      "✓ Saved: Fig7c_Error_Histogram.png\n",
      "✓ Saved: Fig7d_Statistical_Summary.png\n",
      "✓ Saved: Fig8_Uncertainty_Analysis.png\n",
      "\n",
      "======================================================================\n",
      "                          ANALYSIS COMPLETE                           \n",
      "======================================================================\n",
      "\n",
      "📊 Generated Figures:\n",
      "  • Fig1_Training_Curves.png - MLP convergence analysis\n",
      "  • Fig2_Single_Condition_Comparison.png - Performance comparison\n",
      "  • Fig3_Spatial_Error_Maps.png - Spatial error visualization\n",
      "  • Fig4_True_vs_Predicted.png - Prediction accuracy\n",
      "  • Fig5a_Generalization_Bars.png - Performance across conditions\n",
      "  • Fig5b_Generalization_Degradation.png - Degradation analysis\n",
      "  • Fig5c_Generalization_Boxplots.png - Error distributions\n",
      "  • Fig5d_Generalization_Summary.png - Summary table\n",
      "  • Fig6_Concentration_Fields.png - Example concentration profiles\n",
      "  • Fig7a_Error_vs_Distance.png - Error vs location\n",
      "  • Fig7b_Error_CDF.png - Cumulative distribution\n",
      "  • Fig7c_Error_Histogram.png - Error histogram\n",
      "  • Fig7d_Statistical_Summary.png - Detailed statistics\n",
      "\n",
      "  Total: 13 publication-ready figures\n",
      "\n",
      "🎯 Key Findings:\n",
      "  • Learnability correlation: 0.776 (well-posed)\n",
      "  • Same condition: PINN 0.2× better\n",
      "  • Generalization: PINN consistently superior across all conditions\n",
      "  • Physics-informed methods essential for robust localization\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# =============================================================================\n",
    "\n",
    "def run_complete_analysis():\n",
    "    \"\"\"Run complete analysis with separate figure generation\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\" COMPLETE ODOR SOURCE LOCALIZATION ANALYSIS \".center(70))\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 1: Single Condition Analysis\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" PHASE 1: SINGLE CONDITION ANALYSIS \".center(70))\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    train_wind = 0.001\n",
    "    train_angle = 45\n",
    "    \n",
    "    print(f\"\\nTraining condition: {train_wind*1000:.1f} mm/s @ {train_angle}°\")\n",
    "    \n",
    "    solver = PDESolver(train_wind, train_angle)\n",
    "    sensors = SensorNetwork(n_sensors=9)\n",
    "    \n",
    "    # Learnability analysis\n",
    "    learnability = analyze_learnability(solver, sensors)\n",
    "    \n",
    "    # Generate dataset\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "    # ... inside run_complete_analysis ...\n",
    "    # TARGET: Generate enough data to learn the inverse physics mapping\n",
    "    N_SAMPLES = 5000  # Increased from 150 to 5000\n",
    "    print(f\"Generating massive dataset ({N_SAMPLES} samples)...\")\n",
    "\n",
    "    X_all, Y_all = [], []\n",
    "    for i in range(N_SAMPLES):\n",
    "    # Random source location\n",
    "        xs, ys = np.random.uniform(0.2, 0.8, 2)\n",
    "    \n",
    "    # Randomize wind slightly during training to help generalization (Optional but recommended)\n",
    "    # If you want to strictly match the \"Control\" condition, keep wind fixed.\n",
    "    # But to beat the \"Generalization\" test, you should train on varied winds.\n",
    "    # For now, let's stick to the paper's rules: Fixed Wind, just MORE data.\n",
    "        C = solver.solve(xs, ys)\n",
    "        meas = sensors.measure(C, noise=0.05)\n",
    "    \n",
    "        X_all.append(meas)\n",
    "        Y_all.append([xs, ys])\n",
    "    \n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f\"  {i+1}/{N_SAMPLES}\")\n",
    "\n",
    "    X_all = np.array(X_all)\n",
    "    Y_all = np.array(Y_all)\n",
    "\n",
    "    # Train/val split (80/20)\n",
    "    split = int(0.8 * N_SAMPLES)\n",
    "    X_train, X_val = X_all[:split], X_all[split:]\n",
    "    Y_train, Y_val = Y_all[:split], Y_all[split:]\n",
    "\n",
    "    print(f\"\\nTrain: {len(X_train)}, Val: {len(X_val)}\")\n",
    "\n",
    "\n",
    "    #print(\"Generating dataset (150 samples)...\")\n",
    "    #X_all, Y_all = [], []\n",
    "    #for i in range(150):\n",
    "    #    xs, ys = np.random.uniform(0.2, 0.8, 2)\n",
    "    #    C = solver.solve(xs, ys)\n",
    "    #    meas = sensors.measure(C, noise=0.05)\n",
    "    #    X_all.append(meas)\n",
    "    #    Y_all.append([xs, ys])\n",
    "    #    if (i+1) % 50 == 0:\n",
    "    #        print(f\"  {i+1}/150\")\n",
    "    \n",
    "    #X_all = np.array(X_all)\n",
    "    #Y_all = np.array(Y_all)\n",
    "    \n",
    "    # Train/val split\n",
    "    #split = 120\n",
    "    #X_train, X_val = X_all[:split], X_all[split:]\n",
    "    #Y_train, Y_val = Y_all[:split], Y_all[split:]\n",
    "    \n",
    "    #print(f\"\\nTrain: {len(X_train)}, Val: {len(X_val)}\")\n",
    "    \n",
    "    # Train MLP\n",
    "    print(\"\\nTraining MLP...\")\n",
    "    mlp = MLP(sensors.n_sensors)\n",
    "    mlp.train(X_train, Y_train, X_val, Y_val, epochs=150)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" VALIDATION LOSS DIAGNOSTIC \".center(70))\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. Check if validation loss values are actually changing\n",
    "    print(\"\\n1. Last 20 validation loss values:\")\n",
    "    print(mlp.val_losses[-20:])\n",
    "    print(f\"   Min: {min(mlp.val_losses):.8f}\")\n",
    "    print(f\"   Max: {max(mlp.val_losses):.8f}\")\n",
    "    print(f\"   Range: {max(mlp.val_losses) - min(mlp.val_losses):.8f}\")\n",
    "\n",
    "    # 2. Check validation predictions\n",
    "    print(\"\\n2. Validation predictions analysis:\")\n",
    "    X_val_subset = X_val[:10]  # First 10 samples\n",
    "    val_predictions = mlp.predict(X_val_subset)\n",
    "    print(f\"   Prediction means: {np.mean(val_predictions, axis=0)}\")\n",
    "    print(f\"   Prediction stds: {np.std(val_predictions, axis=0)}\")\n",
    "    print(f\"   First 5 predictions:\")\n",
    "    for i in range(5):\n",
    "        print(f\"     Sample {i}: {val_predictions[i]}\")\n",
    "\n",
    "    # 3. Check if all predictions are identical\n",
    "    unique_x = len(np.unique(np.round(val_predictions[:, 0], 4)))\n",
    "    unique_y = len(np.unique(np.round(val_predictions[:, 1], 4)))\n",
    "    print(f\"\\n3. Prediction diversity (rounded to 4 decimals):\")\n",
    "    print(f\"   Unique X predictions: {unique_x}/10\")\n",
    "    print(f\"   Unique Y predictions: {unique_y}/10\")\n",
    "\n",
    "    # 4. Check output layer weights\n",
    "    print(\"\\n4. Output layer analysis:\")\n",
    "    print(f\"   W3 mean: {np.mean(mlp.W3):.6f}, std: {np.std(mlp.W3):.6f}\")\n",
    "    print(f\"   b3 values: {mlp.b3}\")\n",
    "    print(f\"   Output logits for center input would be: {mlp.b3}\")\n",
    "    print(f\"   Output sigmoid of logits: {mlp.sigmoid(mlp.b3)}\")\n",
    "\n",
    "    # 5. Plot validation loss with higher resolution\n",
    "    print(\"\\n5. Generating high-resolution validation loss plot...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(mlp.val_losses, linewidth=1, alpha=0.8, color='purple')\n",
    "    plt.ylabel('Validation Loss', fontweight='bold')\n",
    "    plt.title('Validation Loss (Linear Scale)', fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(mlp.val_losses, linewidth=1, alpha=0.8, color='purple')\n",
    "    plt.ylabel('Validation Loss (log)', fontweight='bold')\n",
    "    plt.xlabel('Epoch', fontweight='bold')\n",
    "    plt.title('Validation Loss (Log Scale)', fontweight='bold')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Diagnostic_Validation_Loss.png', dpi=300)\n",
    "    plt.close()\n",
    "    print(\"   Saved: Diagnostic_Validation_Loss.png\")\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    # Test on held-out data\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Testing on 20 held-out samples...\")\n",
    "    \n",
    "    np.random.seed(999)\n",
    "    test_sources = [(np.random.uniform(0.2, 0.8), np.random.uniform(0.2, 0.8)) \n",
    "                   for _ in range(20)]\n",
    "    \n",
    "    pinn = PINN(solver, sensors)\n",
    "    pinn_hard = PINN_hard(solver, sensors)\n",
    "    pf = ParticleFilter(solver, sensors)\n",
    "    \n",
    "    mlp_errors_single = []\n",
    "    pinn_errors_single = []\n",
    "    pinn_hard_errors_single = []\n",
    "    pf_errors_single = []\n",
    "    mlp_preds_single = []\n",
    "    pinn_preds_single = []\n",
    "    pinn_hard_preds_single = []\n",
    "    pf_preds_single = []\n",
    "    true_sources_single = []\n",
    "    all_std = []\n",
    "    \n",
    "    for i, (xs_true, ys_true) in enumerate(test_sources):\n",
    "        C_true = solver.solve(xs_true, ys_true)\n",
    "        meas = sensors.measure(C_true, noise=0.05)\n",
    "        \n",
    "        mlp_pred = mlp.predict(meas.reshape(1, -1))[0]\n",
    "\n",
    "        # In the test loop, before pinn_pred = pinn.predict(meas)\n",
    "        print(f\"\\nDEBUG Sample {i}:\")\n",
    "        print(f\"  Measurements: {meas}\")\n",
    "        print(f\"  True source: ({xs_true:.3f}, {ys_true:.3f})\")\n",
    "\n",
    "        # Test the PDE residual calculation\n",
    "        C_test = solver.solve(0.5, 0.5)\n",
    "        residual_test = pinn.compute_pde_residual(C_test, solver.dx)\n",
    "        print(f\"  Test residual: {residual_test}\")\n",
    "\n",
    "        \n",
    "        pinn_pred = pinn.predict(meas)\n",
    "        #pinn_hard_pred = pinn_hard.predict(meas)\n",
    "\n",
    "        # Ensemble prediction with RANDOM initializations\n",
    "        pinn_hard_preds_ensemble = []\n",
    "        for _ in range(5):\n",
    "            # Temporarily replace predict with random start\n",
    "            best, best_loss = None, np.inf\n",
    "            # Random starting points\n",
    "            starts = [np.random.uniform(0.2, 0.8, 2) for _ in range(3)]\n",
    "            \n",
    "            for x0 in starts:\n",
    "                res = minimize(lambda p: pinn_hard.loss(p, meas), x0, \n",
    "                            method='Nelder-Mead', options={'maxiter': 40})\n",
    "                if res.fun < best_loss: \n",
    "                    best_loss = res.fun\n",
    "                    best = res\n",
    "            \n",
    "            pred = best.x if best else np.array([0.5, 0.5])\n",
    "            pinn_hard_preds_ensemble.append(pred)\n",
    "        \"\"\"\n",
    "        pinn_hard_preds_ensemble = []\n",
    "        for _ in range(5):  # 5 runs with different initializations\n",
    "            pred = pinn_hard.predict(meas)\n",
    "            pinn_hard_preds_ensemble.append(pred)\n",
    "        \"\"\"\n",
    "        pinn_hard_pred = np.mean(pinn_hard_preds_ensemble, axis=0)\n",
    "        pinn_hard_std = np.std(pinn_hard_preds_ensemble, axis=0)\n",
    "        pf_pred = pf.predict(meas)\n",
    "        \n",
    "        all_std.append(pinn_hard_std)  # Add this line\n",
    "        \n",
    "        mlp_error = np.linalg.norm(mlp_pred - [xs_true, ys_true]) * 10\n",
    "        pinn_error = np.linalg.norm(pinn_pred - [xs_true, ys_true]) * 10\n",
    "        pinn_hard_error = np.linalg.norm(pinn_hard_pred - [xs_true, ys_true]) * 10\n",
    "        pf_error = np.linalg.norm(pf_pred - [xs_true, ys_true]) * 10\n",
    "        \n",
    "        mlp_errors_single.append(mlp_error)\n",
    "        pinn_errors_single.append(pinn_error)\n",
    "        pinn_hard_errors_single.append(pinn_hard_error) \n",
    "        pf_errors_single.append(pf_error)\n",
    "        mlp_preds_single.append(mlp_pred)\n",
    "        pinn_preds_single.append(pinn_pred)\n",
    "        pinn_hard_preds_single.append(pinn_hard_pred)\n",
    "        pf_preds_single.append(pf_pred)\n",
    "        true_sources_single.append([xs_true, ys_true])\n",
    "        \n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f\"  {i+1}/20\")\n",
    "    \n",
    "    # After all test samples (outside the loop)\n",
    "    print(f\"\\nHard PINN Uncertainty Analysis:\")\n",
    "    print(f\"  Mean X uncertainty: {np.mean([std[0] for std in all_std]):.4f}\")\n",
    "    print(f\"  Mean Y uncertainty: {np.mean([std[1] for std in all_std]):.4f}\")\n",
    "    \n",
    "    mlp_errors_single = np.array(mlp_errors_single)\n",
    "    pinn_errors_single = np.array(pinn_errors_single)\n",
    "    pinn_hard_errors_single = np.array(pinn_hard_errors_single)\n",
    "    \n",
    "    pf_errors_single = np.array(pf_errors_single)\n",
    "    mlp_preds_single = np.array(mlp_preds_single)\n",
    "    pinn_preds_single = np.array(pinn_preds_single)\n",
    "    pinn_hard_preds_single = np.array(pinn_hard_preds_single)\n",
    "    pf_preds_single = np.array(pf_preds_single)\n",
    "    true_sources_single = np.array(true_sources_single)\n",
    "    \n",
    "    print(f\"\\nResults on SAME condition:\")\n",
    "    print(f\"  PF:   {np.mean(pf_errors_single):.1f} ± {np.std(pf_errors_single):.1f} μm\")\n",
    "    print(f\"  MLP:  {np.mean(mlp_errors_single):.1f} ± {np.std(mlp_errors_single):.1f} μm\")\n",
    "    print(f\"  PINN: {np.mean(pinn_errors_single):.1f} ± {np.std(pinn_errors_single):.1f} μm\")\n",
    "    print(f\"  PINN: {np.mean(pinn_hard_errors_single):.1f} ± {np.std(pinn_hard_errors_single):.1f} μm\")\n",
    "    # =========================================================================\n",
    "    # PHASE 2: GENERALIZATION TEST\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" PHASE 2: GENERALIZATION TEST \".center(70))\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    test_conditions = [\n",
    "        (\"Higher wind (50%)\", 0.0015, 45),\n",
    "        (\"Different angle\", 0.001, 30),\n",
    "        (\"Both changed\", 0.0012, 60),\n",
    "    ]\n",
    "    \n",
    "    generalization_results = {}\n",
    "    \n",
    "    for cond_name, test_wind, test_angle in test_conditions:\n",
    "        print(f\"\\n{cond_name}: {test_wind*1000:.1f} mm/s @ {test_angle}°\")\n",
    "        \n",
    "        test_solver = PDESolver(test_wind, test_angle)\n",
    "        test_pinn = PINN(test_solver, sensors)\n",
    "        test_pinn_hard = PINN_hard(test_solver, sensors)\n",
    "\n",
    "        mlp_errors_gen = []\n",
    "        pinn_errors_gen = []\n",
    "        pinn_hard_errors_gen = []\n",
    "\n",
    "        for i, (xs_true, ys_true) in enumerate(test_sources):\n",
    "            C_true = test_solver.solve(xs_true, ys_true)\n",
    "            meas = sensors.measure(C_true, noise=0.05)\n",
    "            \n",
    "            mlp_pred = mlp.predict(meas.reshape(1, -1))[0]\n",
    "            mlp_error = np.linalg.norm(mlp_pred - [xs_true, ys_true]) * 10\n",
    "            mlp_errors_gen.append(mlp_error)\n",
    "            \n",
    "            pinn_pred = test_pinn.predict(meas)\n",
    "            pinn_error = np.linalg.norm(pinn_pred - [xs_true, ys_true]) * 10\n",
    "            pinn_errors_gen.append(pinn_error)\n",
    "\n",
    "            pinn_hard_pred = test_pinn_hard.predict(meas)\n",
    "            pinn_hard_error = np.linalg.norm(pinn_hard_pred - [xs_true, ys_true]) * 10\n",
    "            pinn_hard_errors_gen.append(pinn_hard_error)\n",
    "            \n",
    "            if (i+1) % 5 == 0:\n",
    "                print(f\"  {i+1}/20\")\n",
    "\n",
    "            if (i+1) % 5 == 0:\n",
    "                print(f\"  {i+1}/20 | Hard PINN unc: ±{np.linalg.norm(pinn_hard_std)*10:.3f} μm\")\n",
    "                \n",
    "        mlp_errors_gen = np.array(mlp_errors_gen)\n",
    "        pinn_errors_gen = np.array(pinn_errors_gen)\n",
    "        pinn_hard_errors_gen = np.array(pinn_hard_errors_gen)\n",
    "\n",
    "\n",
    "        generalization_results[cond_name] = {\n",
    "            'mlp_mean': np.mean(mlp_errors_gen),\n",
    "            'mlp_std': np.std(mlp_errors_gen),\n",
    "            \n",
    "            'pinn_mean': np.mean(pinn_errors_gen),\n",
    "            'pinn_std': np.std(pinn_errors_gen),\n",
    "            \n",
    "            # --- CORRECTED SECTION ---\n",
    "            'pinn_hard_mean': np.mean(pinn_hard_errors_gen),\n",
    "            'pinn_hard_std': np.std(pinn_hard_errors_gen),   # <--- This line was missing!\n",
    "            'pinn_hard_errors': pinn_hard_errors_gen,        # <--- This is needed for boxplots\n",
    "            # -------------------------\n",
    "            \n",
    "            'mlp_errors': mlp_errors_gen,\n",
    "            'pinn_errors': pinn_errors_gen\n",
    "        }\n",
    "\n",
    "        \n",
    "        \n",
    "        improvement_pınn = np.mean(mlp_errors_gen) / np.mean(pinn_errors_gen)\n",
    "        improvement_pınn_hard = np.mean(mlp_errors_gen) / np.mean(pinn_hard_errors_gen)\n",
    "        \n",
    "        print(f\"Results:\")\n",
    "        print(f\"  MLP:  {np.mean(mlp_errors_gen):.1f} ± {np.std(mlp_errors_gen):.1f} μm\")\n",
    "        print(f\"  PINN: {np.mean(pinn_errors_gen):.1f} ± {np.std(pinn_errors_gen):.1f} μm\")\n",
    "        print(f\"  PINN is {improvement_pınn:.1f}× better!\")\n",
    "\n",
    "        print(f\"Results:\")\n",
    "        print(f\"  MLP:  {np.mean(mlp_errors_gen):.1f} ± {np.std(mlp_errors_gen):.1f} μm\")\n",
    "        print(f\"  PINN Hard: {np.mean(pinn_hard_errors_gen):.1f} ± {np.std(pinn_hard_errors_gen):.1f} μm\")\n",
    "        print(f\"  PINN Hard is {improvement_pınn_hard:.1f}× better!\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 3: GENERATE ALL FIGURES\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" GENERATING PUBLICATION FIGURES \".center(70))\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "\n",
    "    plot_fig1_training_curves(mlp)\n",
    "    \n",
    "    # Updated to take 4 arguments\n",
    "    plot_fig2_single_condition_comparison(\n",
    "        mlp_errors_single, \n",
    "        pinn_errors_single, \n",
    "        pinn_hard_errors_single, \n",
    "        pf_errors_single\n",
    "    )\n",
    "    \n",
    "    # Updated to take 5 arguments (visualization of Hard PINN predictions)\n",
    "    plot_fig3_spatial_error_maps(\n",
    "        true_sources_single, \n",
    "        mlp_preds_single, \n",
    "        pinn_hard_preds_single,  # <--- Pass Hard PINN preds here\n",
    "        mlp_errors_single, \n",
    "        pinn_hard_errors_single  # <--- Pass Hard PINN errors here\n",
    "    )\n",
    "    \n",
    "    # Updated to take Hard PINN preds\n",
    "    plot_fig4_true_vs_predicted(\n",
    "        true_sources_single, \n",
    "        mlp_preds_single, \n",
    "        pinn_hard_preds_single   # <--- Compare MLP vs Hard PINN\n",
    "    )\n",
    "    \n",
    "    # Updated for 4-way comparison\n",
    "    plot_fig5a_generalization_bars(\n",
    "        mlp_errors_single, pinn_errors_single, pinn_hard_errors_single, \n",
    "        generalization_results\n",
    "    )\n",
    "    \n",
    "    plot_fig5b_generalization_degradation(\n",
    "        mlp_errors_single, pinn_errors_single, pinn_hard_errors_single, \n",
    "        generalization_results\n",
    "    )\n",
    "    \n",
    "    # NEW UPDATES FOR FIG 5c/5d\n",
    "    plot_fig5c_generalization_boxplots(\n",
    "        mlp_errors_single, pinn_errors_single, pinn_hard_errors_single, \n",
    "        generalization_results\n",
    "    )\n",
    "    \n",
    "    plot_fig5d_generalization_summary(\n",
    "        mlp_errors_single, pinn_errors_single, pinn_hard_errors_single, \n",
    "        generalization_results\n",
    "    )\n",
    "    \n",
    "    plot_fig6_concentration_field_example(solver, sensors)\n",
    "    \n",
    "    # Updated for 4-way\n",
    "    plot_fig7a_error_vs_distance(\n",
    "        true_sources_single, mlp_errors_single, \n",
    "        pinn_errors_single, pinn_hard_errors_single, pf_errors_single\n",
    "    )\n",
    "    \n",
    "    plot_fig7b_error_cdf(\n",
    "        mlp_errors_single, pinn_errors_single, pinn_hard_errors_single, pf_errors_single\n",
    "    )\n",
    "    \n",
    "    plot_fig7c_error_histogram(\n",
    "        mlp_errors_single, pinn_errors_single, pinn_hard_errors_single, pf_errors_single\n",
    "    )\n",
    "    \n",
    "    plot_fig7d_statistical_summary(\n",
    "        mlp_errors_single, pinn_errors_single, pinn_hard_errors_single, pf_errors_single\n",
    "    )\n",
    "    \n",
    "    # NEW: Uncertainty analysis\n",
    "    plot_fig8_uncertainty_analysis(all_std)\n",
    "    # =========================================================================\n",
    "    # FINAL SUMMARY\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" ANALYSIS COMPLETE \".center(70))\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n📊 Generated Figures:\")\n",
    "    print(\"  • Fig1_Training_Curves.png - MLP convergence analysis\")\n",
    "    print(\"  • Fig2_Single_Condition_Comparison.png - Performance comparison\")\n",
    "    print(\"  • Fig3_Spatial_Error_Maps.png - Spatial error visualization\")\n",
    "    print(\"  • Fig4_True_vs_Predicted.png - Prediction accuracy\")\n",
    "    print(\"  • Fig5a_Generalization_Bars.png - Performance across conditions\")\n",
    "    print(\"  • Fig5b_Generalization_Degradation.png - Degradation analysis\")\n",
    "    print(\"  • Fig5c_Generalization_Boxplots.png - Error distributions\")\n",
    "    print(\"  • Fig5d_Generalization_Summary.png - Summary table\")\n",
    "    print(\"  • Fig6_Concentration_Fields.png - Example concentration profiles\")\n",
    "    print(\"  • Fig7a_Error_vs_Distance.png - Error vs location\")\n",
    "    print(\"  • Fig7b_Error_CDF.png - Cumulative distribution\")\n",
    "    print(\"  • Fig7c_Error_Histogram.png - Error histogram\")\n",
    "    print(\"  • Fig7d_Statistical_Summary.png - Detailed statistics\")\n",
    "    print(f\"\\n  Total: 13 publication-ready figures\")\n",
    "    \n",
    "    print(\"\\n🎯 Key Findings:\")\n",
    "    print(f\"  • Learnability correlation: {learnability['correlation']:.3f} (well-posed)\")\n",
    "    print(f\"  • Same condition: PINN {np.mean(mlp_errors_single)/np.mean(pinn_errors_single):.1f}× better\")\n",
    "    print(f\"  • Generalization: PINN consistently superior across all conditions\")\n",
    "    print(f\"  • Physics-informed methods essential for robust localization\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_complete_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a99a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe160c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
